{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\cscat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen as Ureq\n",
    "import urllib.request\n",
    "import requests\n",
    "import PyPDF2\n",
    "from analysis import tokenizer, twitter_sent_analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitepapersDB():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://whitepaperdatabase.com/?s='\n",
    "\n",
    "\n",
    "    def base_page(self, term):\n",
    "        new_url = self.base_url + term\n",
    "        print(new_url)\n",
    "        uClient = Ureq(new_url)\n",
    "        raw_content = uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup = soup(raw_content)\n",
    "        return page_soup\n",
    "\n",
    "    def get_paper_url(self, page_soup):\n",
    "        containers = page_soup.findAll(\"a\")\n",
    "        url = containers[8]['href']\n",
    "        return url\n",
    "\n",
    "    def get_pdf_link(self, paper_url):\n",
    "        uClient = Ureq(paper_url)\n",
    "        raw_content = uClient.read()\n",
    "        uClient.close()\n",
    "        page_soup = soup(raw_content)\n",
    "        pdf_link = page_soup.findAll(\"a\", {\"class\":\"pdfemb-viewer\"})\n",
    "        return pdf_link[0]['href']\n",
    "\n",
    "    def get_pdf(self, ticker, pdf_link):\n",
    "        filename = '../../data/whitepapers/' + ticker + '_whitepaper.pdf'\n",
    "        urllib.request.urlretrieve(pdf_link, filename)\n",
    "\n",
    "    def read_pdf(self, ticker):\n",
    "        corpus = ''\n",
    "        filename = '../../data/whitepapers/' + ticker + '_whitepaper.pdf'\n",
    "        pdf_obj = open(filename, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdf_obj)\n",
    "        pages = pdfReader.numPages\n",
    "        for i in range(pages):\n",
    "            raw_text = pdfReader.getPage(i)\n",
    "            corpus = corpus + raw_text.extractText()\n",
    "        return corpus\n",
    "    \n",
    "    def preprocess_sent(text):\n",
    "        tokens = tokenizer(text)\n",
    "        corpus = [corpus + ' ' + word for word in tokens]\n",
    "        return corpus[0]\n",
    "\n",
    "    def check_sent(self, corpus):\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        sentiment = analyzer.polarity_scores(corpus)\n",
    "        return sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
