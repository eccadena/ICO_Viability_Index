{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at Tweepy\n",
    "#### URL - https://www.pythoncentral.io/introduction-to-tweepy-twitter-for-python/\n",
    "#### credit to Ahmet Novalic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(_api=<tweepy.api.API object at 0x000001EA9AD2F128>, _json={'created_at': 'Mon Nov 11 20:28:45 +0000 2019', 'id': 1193988959703126022, 'id_str': '1193988959703126022', 'text': 'Testing functionality', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': []}, 'source': '<a href=\"https://www.epiclootsmining.com/\" rel=\"nofollow\">ICO_viability</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 1153340693529714688, 'id_str': '1153340693529714688', 'name': 'Eric Cadena', 'screen_name': 'EricCadena13', 'location': '', 'description': 'Hard worker, educator, and life long learner', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 6, 'listed_count': 0, 'created_at': 'Mon Jul 22 16:27:04 +0000 2019', 'favourites_count': 4, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 6, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'can_media_tag': True, 'followed_by': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'en'}, created_at=datetime.datetime(2019, 11, 11, 20, 28, 45), id=1193988959703126022, id_str='1193988959703126022', text='Testing functionality', truncated=False, entities={'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': []}, source='ICO_viability', source_url='https://www.epiclootsmining.com/', in_reply_to_status_id=None, in_reply_to_status_id_str=None, in_reply_to_user_id=None, in_reply_to_user_id_str=None, in_reply_to_screen_name=None, author=User(_api=<tweepy.api.API object at 0x000001EA9AD2F128>, _json={'id': 1153340693529714688, 'id_str': '1153340693529714688', 'name': 'Eric Cadena', 'screen_name': 'EricCadena13', 'location': '', 'description': 'Hard worker, educator, and life long learner', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 6, 'listed_count': 0, 'created_at': 'Mon Jul 22 16:27:04 +0000 2019', 'favourites_count': 4, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 6, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'can_media_tag': True, 'followed_by': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=1153340693529714688, id_str='1153340693529714688', name='Eric Cadena', screen_name='EricCadena13', location='', description='Hard worker, educator, and life long learner', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1, friends_count=6, listed_count=0, created_at=datetime.datetime(2019, 7, 22, 16, 27, 4), favourites_count=4, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=6, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=False, default_profile=True, default_profile_image=False, can_media_tag=True, followed_by=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), user=User(_api=<tweepy.api.API object at 0x000001EA9AD2F128>, _json={'id': 1153340693529714688, 'id_str': '1153340693529714688', 'name': 'Eric Cadena', 'screen_name': 'EricCadena13', 'location': '', 'description': 'Hard worker, educator, and life long learner', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 1, 'friends_count': 6, 'listed_count': 0, 'created_at': 'Mon Jul 22 16:27:04 +0000 2019', 'favourites_count': 4, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 6, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'can_media_tag': True, 'followed_by': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, id=1153340693529714688, id_str='1153340693529714688', name='Eric Cadena', screen_name='EricCadena13', location='', description='Hard worker, educator, and life long learner', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=1, friends_count=6, listed_count=0, created_at=datetime.datetime(2019, 7, 22, 16, 27, 4), favourites_count=4, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=6, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1153341016243658758/JpYo7dls_normal.jpg', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=False, default_profile=True, default_profile_image=False, can_media_tag=True, followed_by=False, following=False, follow_request_sent=False, notifications=False, translator_type='none'), geo=None, coordinates=None, place=None, contributors=None, is_quote_status=False, retweet_count=0, favorite_count=0, favorited=False, retweeted=False, lang='en')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consumer keys and access tokens, used for OAuth\n",
    "consumer_key = os.getenv(\"TWITTER_CONSUMER_API\")\n",
    "consumer_secret = os.getenv(\"TWITTER_CONSUMER_SECRET_API\")\n",
    "access_token = os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "access_token_secret = os.getenv(\"TWITTER_ACCESS_SECRET_TOKEN\")\n",
    " \n",
    "# OAuth process, using the keys and tokens\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    " \n",
    "# Creation of the actual interface, using authentication\n",
    "api = tweepy.API(auth)\n",
    " \n",
    "# Sample method, used to update a status\n",
    "api.update_status('Testing functionality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Eric Cadena\n",
      "Location: \n",
      "Friends: 6\n"
     ]
    }
   ],
   "source": [
    "user = api.me()\n",
    " \n",
    "print('Name: ' + user.name)\n",
    "print('Location: ' + user.location)\n",
    "print('Friends: ' + str(user.friends_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Christian's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_list(topic_of_tweet, num_of_tweets):\n",
    "    '''\n",
    "    Returns a dataframe of the most recent 'N' tweets from Twitter tokenized and counted.\n",
    "\n",
    "    Arguements: `topic_of_tweet` : str; what hashtag is being searched\n",
    "                'num_of_tweets' : int; how many tweet do you want returned\n",
    "    '''\n",
    "    text,time, word_list, word_count=[],[],[],[]\n",
    "    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    for tweet in tweepy.Cursor(api.search, q=topic_of_tweet, tweet_mode='extended').items(num_of_tweets):\n",
    "        text.append(tweet.full_text),\n",
    "        time.append(tweet.created_at)\n",
    "    tweets_df = pd.DataFrame({'Tweet':text}, index=time)\n",
    "    [word_list.append(tokenizer(text)) for text in tweets_df.Tweet] #tokenize \n",
    "    tweets_df['Tokens'] = word_list\n",
    "    [word_count.append(token_count(token)) for token in tweets_df.Tokens]\n",
    "    tweets_df['Word_Count'] = word_count\n",
    "\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-7feee9d496a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_tweets_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BTC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-62-f5892fe154d1>\u001b[0m in \u001b[0;36mget_tweets_list\u001b[1;34m(topic_of_tweet, num_of_tweets)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtweets_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweet\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mtweets_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-f5892fe154d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtweets_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweet\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mtweets_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(readline)\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;31m# built yet and tokenize is imported.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m     \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m     \u001b[0mrl_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[0mempty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tokenize.py\u001b[0m in \u001b[0;36mdetect_encoding\u001b[1;34m(readline)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m     \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBOM_UTF8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mbom_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tokenize.py\u001b[0m in \u001b[0;36mread_or_stop\u001b[1;34m()\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "get_tweets_list(\"BTC\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_term, number_recent_public_posts):\n",
    "    \n",
    "    for tweet in api.search(q={search_term}, lang=\"en\", rpp={number_recent_public_posts}):\n",
    "        print(f\"{tweet.user.name}:{tweet.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane Nichols:RT @NewtonSchoolsKS: USD 373 hosted its Veterans Day Brunch this morning honoring USD 373 employees and retirees who have served in the mil…\n",
      "Champ:TechCrunch: Alibaba’s Singles Day set record of USD 38 billion in a single day. https://t.co/c3RWUYXkOl\n",
      "205M🌍E:RT @sneakerbost: Nike Air Max 90 NRG\n",
      "Color: Desert Sand/Black-Desert Dust\n",
      "Style Code: CI5646-001\n",
      "Release Date: November 15, 2019\n",
      "Price: $13…\n",
      "𝐋𝐢𝐧𝐚 ☂️:RT @gwsbyun: •kpop fans! selling, exo, redvelvet, blackpink, redvelvet, and cherry bullet photocards!!! \n",
      "•each individual photocard is $8 U…\n",
      "syd 2!!!:RT @gwsbyun: •kpop fans! selling, exo, redvelvet, blackpink, redvelvet, and cherry bullet photocards!!! \n",
      "•each individual photocard is $8 U…\n",
      "LittleX 💀🎃:RT @dragvancat: Alright, I'm selling my fat/inflation kink character because I'm short on funds for therapy. Includes his reference sheet,…\n",
      "Limited Edition:Red Dead Redemption -- #SpecialEdition (Sony #PlayStation3, 2010) #eBay #LimitedEdition\n",
      "⏰ Ends in 4h\n",
      "💲 Last Price U… https://t.co/ipLJwANzwW\n",
      "icebox:Discounting the adpotables I put up last week! They're $30 USD each now https://t.co/mMYSk1pTYd\n",
      "Side Projectors:Side project for sale! - $49,000 USD - check out 100% Amazon Fully Managed Passive Cashflow ForSale..… https://t.co/HkK29Tu52f\n",
      "COLDRAMEN:RT @springhakurei: will jeto spend 500 usd on a simon statue, find out in the next episode of dragon ball z\n",
      "Block Watcher:Tue Nov 12 01:38:00 2019 (27:22)\n",
      "USD : 8738.75\n",
      "Wght: 0.44\n",
      "Blk#: 603378\n",
      "Size: 1241.6 KB\n",
      "TXs:  2858\n",
      "Pool: 4024 (1.3 MB)\n",
      "#bitcoin\n",
      "𝐚𝐥𝐞𝐱𝐢𝐚 ♥ ‘𝐬 𝐁𝐓𝐒 & 𝐦𝐢𝐬𝐬𝐞𝐬 𝐭𝐡𝐞𝐦:RT @yirenIvr: bts, bts and bts knowing they have the  highest grossing tour (over 170M USD) for a korean act and sold out stadiums without…\n",
      "Queen Hana 💋:I bought wrong size for halily so nak letgo kasut ni for baby girl 😊\n",
      "\n",
      "Brand new Ralph Lauren (authentic)\n",
      "Price : US… https://t.co/kF6n3BgU1q\n",
      "外入:RT @churi_mon: 🌏EVEO 6th country listing🌏\n",
      "\n",
      "Sleeping Giant Cryptocurrency No.1 exchange\n",
      "\n",
      "■Date■Nov.22 2019 12UTC.(PM9 JST)\n",
      "■Pair■BTC/ETH/USD…\n",
      "FXStreet News:AUD/USD: Mildly positive after better than forecast Aussie NAB Business data https://t.co/ptdm2EuuOM #AUDUSD… https://t.co/yFnWBaac9B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(get_tweets('USD', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'full_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-363b718b96c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'full_text'"
     ]
    }
   ],
   "source": [
    "test.full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geeks for Geeks Sentiment analysis https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-bbdd4b01608a>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-bbdd4b01608a>\"\u001b[1;36m, line \u001b[1;32m34\u001b[0m\n\u001b[1;33m    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#VS CODE For reference. Code is indepth. Christian has working code from previous personal projects. will keep this here for reference if needed later.\n",
    "\n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''\n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'XXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        access_token = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "        access_token_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "### usual authentication steps ------------------------\n",
    "\n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) \n",
    "                                    |(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "### using regex to clean tweets --------------------------\n",
    "                               \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "\n",
    "#cleaning the tweet then passing it to a sentiment analysis function, polarity.\n",
    "                               \n",
    "    def get_tweets(self, query, count = 10): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "  \n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count) \n",
    "  \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "  \n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "  \n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "  \n",
    "            # return parsed tweets \n",
    "            return tweets \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "  \n",
    "def main(): \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    # calling function to get tweets \n",
    "    tweets = api.get_tweets(query = 'Donald Trump', count = 200) \n",
    "  \n",
    "    # picking positive tweets from tweets \n",
    "    ptweets = [tweet for tweet in tweets if tweet['sentiment'] == 'positive'] \n",
    "    # percentage of positive tweets \n",
    "    print(\"Positive tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n",
    "    # picking negative tweets from tweets \n",
    "    ntweets = [tweet for tweet in tweets if tweet['sentiment'] == 'negative'] \n",
    "    # percentage of negative tweets \n",
    "    print(\"Negative tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n",
    "    # percentage of neutral tweets \n",
    "    print(\"Neutral tweets percentage: {} % \\ \n",
    "        \".format(100*len(tweets - ntweets - ptweets)/len(tweets))) \n",
    "  \n",
    "    # printing first 5 positive tweets \n",
    "    print(\"\\n\\nPositive tweets:\") \n",
    "    for tweet in ptweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "    # printing first 5 negative tweets \n",
    "    print(\"\\n\\nNegative tweets:\") \n",
    "    for tweet in ntweets[:10]: \n",
    "        print(tweet['text']) \n",
    "  \n",
    "# if __name__ == \"__main__\": \n",
    "#     # calling main function \n",
    "#     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
